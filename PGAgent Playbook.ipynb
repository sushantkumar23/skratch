{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, n=1000):\n",
    "        \n",
    "        self.n = n\n",
    "        self.num_actions = 3\n",
    "        self.series = 3 + np.sin(0.01* np.arange(self.n))\n",
    "        self.done = False\n",
    "        self.action_space = gym.spaces.Discrete(self.num_actions)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.index += 1\n",
    "        obs = self.series[self.index]\n",
    "        step_return = np.log(self.series[self.index]/self.series[self.index-1])\n",
    "        reward = (action - 1) * step_return\n",
    "        \n",
    "        if self.index >= (self.n-1):\n",
    "            self.done = True\n",
    "            \n",
    "        info = {\n",
    "            'return': step_return\n",
    "        }\n",
    "        return obs, reward, self.done, info\n",
    "        \n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        self.done = False\n",
    "        obs = self.series[self.index]\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent(object):\n",
    "    \n",
    "    def __init__(self, num_actions=3, replay_buffer_size=1000, stack_size=8, l=96, n=10, train_period=20):\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.stack_size = stack_size\n",
    "        self._replay = collections.deque(maxlen=self.replay_buffer_size)\n",
    "        \n",
    "        self.series = []\n",
    "        self.n = n\n",
    "        self.l = l\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.model = self._build_model()\n",
    "\n",
    "\n",
    "    def path_score(self, path):\n",
    "        \"\"\"Path is a list of tuples, each tuple is a state, action pair [(s, a), (s, a)]\n",
    "        Returns the sums of the rewards for the path using the current policy\"\"\"\n",
    "        \n",
    "        path_reward = 0\n",
    "        for path_step in path:\n",
    "            for experience in self._replay:\n",
    "                if path_step == (experience[0], experience[1]):\n",
    "                    path_reward += experience[2]\n",
    "        \n",
    "        return path_reward\n",
    "        \n",
    "    def log_prob(self, state, action):\n",
    "        p = K.log(self.model(state))\n",
    "        p = K.slice(p, [action], [1])\n",
    "        return p\n",
    "    \n",
    "    def path_loss(self, path):\n",
    "        path_loss = K.constant(0)\n",
    "        for step in path:\n",
    "            state = step[0]\n",
    "            action = step[1]\n",
    "            path_loss += self.log_prob(state, action)\n",
    "            \n",
    "        path_loss *= self.path_score(path)\n",
    "        path_loss = -path_loss\n",
    "        \n",
    "        return path_loss\n",
    "    \n",
    "    def build_path(self, start_state):\n",
    "        \n",
    "        path = []\n",
    "        state = start_state\n",
    "        action = self._select_action(state)\n",
    "\n",
    "        path.append((state, action))\n",
    "        length = 1\n",
    "        \n",
    "        while length < self.l:\n",
    "            for experience in self.replay:\n",
    "                if (state, action) == (experience[0], experience[1]):\n",
    "                    state = experience[3]\n",
    "                    action = self._select_action(state)\n",
    "                    length += 1\n",
    "                    path.append((state, action))\n",
    "            \n",
    "        return self.path_loss(path)\n",
    "        \n",
    "\n",
    "    def loss_computation(self):\n",
    "        l = K.constant(0)\n",
    "        samples_index = random.sample(range(len(self._replay) - self.l), self.n)\n",
    "        print(samples_index)\n",
    "\n",
    "        for i in samples_index:\n",
    "            print(\"Current_index: {}\".format(i))\n",
    "            experience = self._replay[i]\n",
    "            start_state = experience[0]\n",
    "            l += self.build_path(start_state)\n",
    "\n",
    "        l = l/self.n\n",
    "            \n",
    "        return l\n",
    "    \n",
    "    def loss_keras(self):\n",
    "        \n",
    "        def loss(y_true, y_pred):\n",
    "            l = self.loss_computation()\n",
    "            return l\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def _train_step(self):\n",
    "        \n",
    "        # Train whenever a multiple of train_period\n",
    "        if (self.steps % self.train_period) == 0 and (len(self._replay) >= max(self.n, self.l)):\n",
    "            tensor2 = K.const(np.random.randn((8,2)))\n",
    "            tensor = K.const(np.array([1,2,3]))\n",
    "            self.model.fit(tensor2, tensor)\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        \n",
    "        inputs = Input(shape=(self.stack_size,))\n",
    "        X = Dense(64, activation='relu')(inputs)\n",
    "        X = Dense(64, activation='relu')(X)\n",
    "        predictions = Dense(self.num_actions, activation='softmax')(X)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        loss = self.loss_keras()\n",
    "        model.compile(optimizer='adam', loss=loss)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def initialise_episode(self, observation):\n",
    "        self.series.extend([observation] * self.stack_size)\n",
    "        self.state = self.construct_state(observation)\n",
    "        return self._select_action(self.state)\n",
    "    \n",
    "    def construct_state(self, observation):\n",
    "        self.series.append(observation)\n",
    "        log_ret = np.log((np.array(self.series[1:]))/(np.array(self.series[:-1])))\n",
    "        return log_ret[-self.stack_size:]\n",
    "    \n",
    "    def step(self, reward, observation):\n",
    "        self.current_state = self.construct_state(observation)\n",
    "        experience = (self.state, self.action, reward, self.current_state)\n",
    "        self._replay.append(experience)\n",
    "        self.state = self.current_state\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Train the model\n",
    "        self._train_step()\n",
    "        return self._select_action(self.state)\n",
    "    \n",
    "    def _select_action(self, state):\n",
    "        state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        state_tensor = tf.reshape(state_tensor, shape=(1, self.stack_size))\n",
    "\n",
    "        action_prob_values = K.eval(self.model(state_tensor))\n",
    "        print(\"Action prob values: {}\".format(action_prob_values))\n",
    "        self.action = np.random.choice(a=np.arange(self.num_actions), p=action_prob_values[0])\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-0b40e3055377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSineEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyGradientAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-180-c1f125a907c5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_actions, replay_buffer_size, stack_size, l, n, train_period)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-c1f125a907c5>\u001b[0m in \u001b[0;36m_build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 830\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    831\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \"\"\"\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-c1f125a907c5>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_computation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-c1f125a907c5>\u001b[0m in \u001b[0;36mloss_computation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss_computation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0msamples_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "env = SineEnv(n=1000)\n",
    "agent = PolicyGradientAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action prob values: [[0.33333334 0.33333334 0.33333334]]\n",
      "Action prob values: [[0.33348563 0.33326626 0.33324805]]\n",
      "Action prob values: [[0.33336577 0.33330396 0.33333027]]\n",
      "Action prob values: [[0.33346197 0.33338934 0.33314866]]\n",
      "Action prob values: [[0.3335409  0.33334595 0.33311316]]\n",
      "Action prob values: [[0.3335725  0.33333716 0.33309036]]\n",
      "Action prob values: [[0.33360684 0.3334612  0.3329319 ]]\n",
      "Action prob values: [[0.33359668 0.33345664 0.33294672]]\n",
      "Action prob values: [[0.33354747 0.3335061  0.33294642]]\n",
      "Action prob values: [[0.33354664 0.33350548 0.33294788]]\n",
      "Action prob values: [[0.3335458  0.33350483 0.3329494 ]]\n",
      "Action prob values: [[0.3335449  0.33350414 0.33295092]]\n",
      "Action prob values: [[0.33354405 0.33350345 0.33295247]]\n",
      "Action prob values: [[0.33354318 0.3335028  0.33295408]]\n",
      "Action prob values: [[0.33354226 0.33350208 0.33295566]]\n",
      "Action prob values: [[0.33354136 0.33350137 0.3329573 ]]\n",
      "Action prob values: [[0.3335404  0.33350062 0.33295894]]\n",
      "Action prob values: [[0.3335395  0.3334999  0.33296064]]\n",
      "Action prob values: [[0.33353853 0.33349913 0.33296233]]\n",
      "Action prob values: [[0.33353758 0.33349836 0.33296406]]\n",
      "Action prob values: [[0.33353662 0.3334976  0.33296585]]\n",
      "Action prob values: [[0.33353564 0.3334968  0.3329676 ]]\n",
      "Action prob values: [[0.33353463 0.33349597 0.33296943]]\n",
      "Action prob values: [[0.3335336  0.33349517 0.33297125]]\n",
      "Action prob values: [[0.33353257 0.33349434 0.33297306]]\n",
      "Action prob values: [[0.33353156 0.3334935  0.33297497]]\n",
      "Action prob values: [[0.3335305  0.33349264 0.33297682]]\n",
      "Action prob values: [[0.33352944 0.3334918  0.33297876]]\n",
      "Action prob values: [[0.33352837 0.33349094 0.3329807 ]]\n",
      "Action prob values: [[0.33352727 0.33349004 0.33298263]]\n",
      "Action prob values: [[0.3335262  0.33348918 0.33298466]]\n",
      "Action prob values: [[0.3335251  0.3334883  0.33298665]]\n",
      "Action prob values: [[0.33352396 0.33348736 0.33298865]]\n",
      "Action prob values: [[0.33352286 0.33348647 0.33299074]]\n",
      "Action prob values: [[0.33352172 0.33348557 0.33299276]]\n",
      "Action prob values: [[0.33352056 0.33348462 0.33299482]]\n",
      "Action prob values: [[0.3335194  0.33348367 0.33299693]]\n",
      "Action prob values: [[0.3335182  0.3334827  0.33299902]]\n",
      "Action prob values: [[0.33351704 0.3334818  0.33300117]]\n",
      "Action prob values: [[0.33351585 0.33348083 0.3330033 ]]\n",
      "Action prob values: [[0.33351466 0.33347985 0.3330055 ]]\n",
      "Action prob values: [[0.33351347 0.3334789  0.3330077 ]]\n",
      "Action prob values: [[0.33351225 0.3334779  0.3330099 ]]\n",
      "Action prob values: [[0.33351102 0.33347693 0.3330121 ]]\n",
      "Action prob values: [[0.33350977 0.33347592 0.33301434]]\n",
      "Action prob values: [[0.33350852 0.3334749  0.33301657]]\n",
      "Action prob values: [[0.33350727 0.3334739  0.33301884]]\n",
      "Action prob values: [[0.33350602 0.33347288 0.33302113]]\n",
      "Action prob values: [[0.33350474 0.33347183 0.33302343]]\n",
      "Action prob values: [[0.33350343 0.3334708  0.3330257 ]]\n",
      "Action prob values: [[0.33350217 0.33346978 0.33302805]]\n",
      "Action prob values: [[0.33350086 0.3334687  0.33303037]]\n",
      "Action prob values: [[0.33349958 0.33346766 0.33303276]]\n",
      "Action prob values: [[0.33349827 0.33346662 0.33303514]]\n",
      "Action prob values: [[0.33349693 0.33346555 0.3330375 ]]\n",
      "Action prob values: [[0.33349565 0.3334645  0.33303994]]\n",
      "Action prob values: [[0.33349428 0.33346337 0.3330423 ]]\n",
      "Action prob values: [[0.33349293 0.3334623  0.33304474]]\n",
      "Action prob values: [[0.33349162 0.33346125 0.3330472 ]]\n",
      "Action prob values: [[0.33349025 0.33346015 0.33304963]]\n",
      "Action prob values: [[0.33348888 0.33345905 0.33305207]]\n",
      "Action prob values: [[0.33348754 0.33345798 0.33305457]]\n",
      "Action prob values: [[0.33348614 0.3334568  0.33305702]]\n",
      "Action prob values: [[0.33348477 0.3334557  0.33305955]]\n",
      "Action prob values: [[0.33348337 0.33345458 0.33306202]]\n",
      "Action prob values: [[0.333482   0.33345348 0.3330646 ]]\n",
      "Action prob values: [[0.3334806  0.33345234 0.3330671 ]]\n",
      "Action prob values: [[0.33347917 0.3334512  0.33306962]]\n",
      "Action prob values: [[0.33347774 0.33345005 0.33307216]]\n",
      "Action prob values: [[0.33347633 0.33344895 0.33307475]]\n",
      "Action prob values: [[0.3334749 0.3334478 0.3330773]]\n",
      "Action prob values: [[0.33347344 0.33344668 0.33307987]]\n",
      "Action prob values: [[0.333472   0.33344555 0.33308247]]\n",
      "Action prob values: [[0.33347055 0.3334444  0.33308506]]\n",
      "Action prob values: [[0.3334691  0.33344322 0.33308765]]\n",
      "Action prob values: [[0.33346763 0.33344206 0.33309028]]\n",
      "Action prob values: [[0.33346617 0.3334409  0.33309287]]\n",
      "Action prob values: [[0.33346474 0.33343977 0.33309555]]\n",
      "Action prob values: [[0.33346325 0.33343858 0.33309817]]\n",
      "Action prob values: [[0.33346176 0.3334374  0.33310083]]\n",
      "Action prob values: [[0.33346027 0.33343622 0.33310345]]\n",
      "Action prob values: [[0.33345878 0.33343506 0.33310613]]\n",
      "Action prob values: [[0.3334573  0.33343387 0.3331088 ]]\n",
      "Action prob values: [[0.33345583 0.3334327  0.33311152]]\n",
      "Action prob values: [[0.3334543 0.3334315 0.3331142]]\n",
      "Action prob values: [[0.3334528 0.3334303 0.3331169]]\n",
      "Action prob values: [[0.3334513  0.33342913 0.33311963]]\n",
      "Action prob values: [[0.33344978 0.3334279  0.3331223 ]]\n",
      "Action prob values: [[0.33344826 0.3334267  0.33312503]]\n",
      "Action prob values: [[0.33344674 0.33342552 0.33312777]]\n",
      "Action prob values: [[0.33344522 0.3334243  0.3331305 ]]\n",
      "Action prob values: [[0.33344367 0.33342305 0.33313322]]\n",
      "Action prob values: [[0.33344215 0.33342186 0.333136  ]]\n",
      "Action prob values: [[0.3334406  0.33342063 0.33313873]]\n",
      "Action prob values: [[0.33343905 0.33341938 0.33314148]]\n",
      "Action prob values: [[0.33343753 0.3334182  0.33314428]]\n",
      "Action prob values: [[0.33343598 0.33341697 0.33314702]]\n",
      "Action prob values: [[0.33343443 0.33341572 0.33314982]]\n",
      "Action prob values: [[0.33343288 0.3334145  0.3331526 ]]\n",
      "Action prob values: [[0.33343133 0.33341327 0.3331554 ]]\n",
      "Action prob values: [[0.33342975 0.33341202 0.33315817]]\n",
      "Action prob values: [[0.3334282 0.3334108 0.333161 ]]\n",
      "Action prob values: [[0.33342665 0.33340958 0.3331638 ]]\n",
      "Action prob values: [[0.33342507 0.3334083  0.33316663]]\n",
      "Action prob values: [[0.3334235  0.33340707 0.3331694 ]]\n",
      "Action prob values: [[0.33342195 0.33340582 0.33317226]]\n",
      "Action prob values: [[0.33342037 0.33340457 0.3331751 ]]\n",
      "Action prob values: [[0.33341876 0.3334033  0.3331779 ]]\n",
      "Action prob values: [[0.3334172  0.33340207 0.33318076]]\n",
      "Action prob values: [[0.3334156 0.3334008 0.3331836]]\n",
      "Action prob values: [[0.33341402 0.33339953 0.33318642]]\n",
      "Action prob values: [[0.33341244 0.33339828 0.33318928]]\n",
      "Action prob values: [[0.33341086 0.33339703 0.33319214]]\n",
      "Action prob values: [[0.33340928 0.33339578 0.333195  ]]\n",
      "Action prob values: [[0.33340767 0.33339447 0.33319786]]\n",
      "Action prob values: [[0.33340606 0.33339322 0.33320072]]\n",
      "Action prob values: [[0.33340448 0.33339193 0.3332036 ]]\n",
      "Action prob values: [[0.33340287 0.33339068 0.33320647]]\n",
      "Action prob values: [[0.33340126 0.3333894  0.33320934]]\n",
      "Action prob values: [[0.33339965 0.33338812 0.33321223]]\n",
      "Action prob values: [[0.33339804 0.33338684 0.3332151 ]]\n",
      "Action prob values: [[0.33339643 0.33338556 0.33321798]]\n",
      "Action prob values: [[0.33339486 0.3333843  0.3332209 ]]\n",
      "Action prob values: [[0.33339325 0.33338302 0.3332238 ]]\n",
      "Action prob values: [[0.33339164 0.33338174 0.3332267 ]]\n",
      "Action prob values: [[0.33338997 0.3333804  0.33322954]]\n",
      "Action prob values: [[0.3333884  0.33337915 0.33323246]]\n",
      "Action prob values: [[0.33338675 0.33337784 0.33323538]]\n",
      "Action prob values: [[0.33338517 0.33337662 0.3332383 ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-5aa376d18362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbenchmark_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-153-85194414690e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, reward, observation)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_select_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-85194414690e>\u001b[0m in \u001b[0;36m_select_action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0maction_prob_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Action prob values: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_prob_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \"\"\"\n\u001b[0;32m--> 663\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \"\"\"\n\u001b[0;32m--> 707\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5211\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5212\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5213\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "observations = []\n",
    "action = agent.initialise_episode(obs)\n",
    "\n",
    "agent_rewards = []\n",
    "benchmark_rewards = []\n",
    "\n",
    "while not done:\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    agent_rewards.append(reward)\n",
    "    benchmark_rewards.append(info['return'])\n",
    "    if not done:\n",
    "        action = agent.step(reward, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x105b095f8>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105b09518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(agent_rewards))\n",
    "plt.plot(np.cumsum(benchmark_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.003316366504332892,\n",
       " 0.0,\n",
       " -0.00329287248224624,\n",
       " -0.0032807539279420604,\n",
       " -0.0032683924580139976,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0032298858318851437,\n",
       " 0.0032165878111498707,\n",
       " 0.0,\n",
       " -0.003189318567616955,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0031467818413767564,\n",
       " -0.0031321806349846053,\n",
       " 0.00311737365092575,\n",
       " 0.003102364060013224,\n",
       " 0.0,\n",
       " 0.0030717495675277174,\n",
       " -0.0030561508341639928,\n",
       " -0.003040361830362619,\n",
       " 0.003024385553510366,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0029586664525747012,\n",
       " -0.0029417975615126063,\n",
       " 0.00292475865698327,\n",
       " 0.0,\n",
       " 0.0028901818002611264,\n",
       " -0.002872649259740262,\n",
       " 0.0,\n",
       " -0.002837109231115252,\n",
       " 0.002819106953446421,\n",
       " -0.0028009532518951706,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0027456086668977133,\n",
       " -0.0027268741702648376,\n",
       " -0.0027080005382589287,\n",
       " 0.0026889901323595075,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0026116269017221576,\n",
       " 0.0025919669544558527,\n",
       " 0.00257218374937059,\n",
       " 0.0,\n",
       " -0.0025322561215376667,\n",
       " 0.002512115902876299,\n",
       " 0.0024918608351444252,\n",
       " 0.002471492947875931,\n",
       " 0.0,\n",
       " -0.002430426690799959,\n",
       " -0.0024097322388941774,\n",
       " -0.002388932803862609,\n",
       " -0.0023680302758902964,\n",
       " 0.0023470265182109857,\n",
       " 0.0023259233674141357,\n",
       " 0.0023047226337578686,\n",
       " 0.0022834261014821108,\n",
       " 0.002262035529122147,\n",
       " -0.002240552649823474,\n",
       " 0.0022189791716577365,\n",
       " 0.002197316777939303,\n",
       " 0.002175567127542045,\n",
       " 0.002153731855216092,\n",
       " -0.002131812571906344,\n",
       " -0.002109810865068535,\n",
       " 0.0,\n",
       " 0.002065566415095601,\n",
       " 0.0,\n",
       " -0.002021010747231964,\n",
       " 0.0019986199347039064,\n",
       " 0.0019761557478812984,\n",
       " -0.0019536196186701583,\n",
       " -0.0019310129580148085,\n",
       " 0.0,\n",
       " -0.0018855935832260981,\n",
       " 0.0018627835889914218,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.001770905714766397,\n",
       " 0.0,\n",
       " 0.001724601928571185,\n",
       " 0.0,\n",
       " -0.0016780680987806065,\n",
       " -0.001654717908825452,\n",
       " -0.0016313137631913642,\n",
       " -0.0016078568117145022,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0015371803764417183,\n",
       " -0.001513523376466299,\n",
       " -0.0014898190797565007,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0013945498240271542,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0012986057157491826,\n",
       " 0.001274521560564857,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0012020485826523322,\n",
       " -0.001177820452277167,\n",
       " -0.001153558573913429,\n",
       " -0.0011292638232563912,\n",
       " 0.0011049370651452083,\n",
       " -0.0010805791538236042,\n",
       " 0.0010561909331957033,\n",
       " 0.0,\n",
       " -0.001007326889482452,\n",
       " -0.0009828527048115377,\n",
       " 0.0009583514881679478,\n",
       " 0.0009338240355757949,\n",
       " -0.0009092711342353905,\n",
       " 0.0008846935627701666,\n",
       " 0.0,\n",
       " 0.0008354674825487633,\n",
       " 0.0,\n",
       " 0.0007861518616731715,\n",
       " -0.0007614623358838022,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0006872756645280893,\n",
       " -0.0006625098054822129,\n",
       " 0.0006377266442642075,\n",
       " -0.0006129268810948215,\n",
       " 0.0005881112104857237,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0005135756183385425,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.00038909481737386584,\n",
       " -0.0003641664045538399,\n",
       " 0.00033922876584603347,\n",
       " -0.0003142825480372292,\n",
       " -0.0002893283949263217,\n",
       " -0.00026436694754709,\n",
       " 0.0,\n",
       " -0.00021442472161826125,\n",
       " -0.00018944521330112508,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -8.948594464775854e-05,\n",
       " 0.0,\n",
       " -3.949036524738611e-05,\n",
       " 1.4490781606556522e-05,\n",
       " -1.050916432767851e-05,\n",
       " -3.550884751830792e-05,\n",
       " 6.050764283606046e-05,\n",
       " 8.55049248407791e-05,\n",
       " 0.00011050006756365475,\n",
       " 0.00013549244428621125,\n",
       " -0.0,\n",
       " 0.0001854663878023749,\n",
       " -0.00021044669544469292,\n",
       " 0.00023542171834554525,\n",
       " 0.0002603908227538602,\n",
       " -0.00028535337285252817,\n",
       " 0.0003103087305360734,\n",
       " 0.0003352562551917403,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.00041004538256876025,\n",
       " -0.0,\n",
       " -0.0004598537580039042,\n",
       " -0.0,\n",
       " 0.0005096151620042937,\n",
       " 0.000534476585915514,\n",
       " -0.0,\n",
       " -0.0005841575111527889,\n",
       " -0.0,\n",
       " 0.0006337779945458605,\n",
       " -0.0,\n",
       " -0.0006833325084126071,\n",
       " -0.0007080832747552626,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.000782221055516829,\n",
       " 0.0008068930617053664,\n",
       " 0.0008315435496400405,\n",
       " -0.000856171773747307,\n",
       " 0.0008807769808910973,\n",
       " -0.0,\n",
       " -0.0009299152924777458,\n",
       " -0.0009544468506520045,\n",
       " 0.0009789522988358874,\n",
       " -0.0,\n",
       " 0.0010278816777907075,\n",
       " -0.001052303992008007,\n",
       " 0.0010766969626266295,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0011496914400250923,\n",
       " 0.0011739586121502959,\n",
       " -0.0,\n",
       " 0.0012223912480268941,\n",
       " -0.0,\n",
       " 0.0012706823156280686,\n",
       " -0.0012947724839022618,\n",
       " 0.0013188245032513806,\n",
       " -0.0013428374293083588,\n",
       " 0.001366810305031816,\n",
       " -0.0013907421604346494,\n",
       " -0.0,\n",
       " 0.001438478863950688,\n",
       " 0.0014622817048828437,\n",
       " -0.0014860395105750772,\n",
       " -0.001509751242163734,\n",
       " -0.0,\n",
       " 0.0015570322542107747,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.001627581389225558,\n",
       " -0.0016509940215551305,\n",
       " 0.0016743528824443778,\n",
       " 0.0016976568080464061,\n",
       " -0.0017209046175773195,\n",
       " -0.0,\n",
       " -0.0017672270788174087,\n",
       " 0.0017902992815888206,\n",
       " 0.0018133104698117828,\n",
       " -0.0,\n",
       " 0.00185914470402523,\n",
       " 0.0018819651535583312,\n",
       " -0.0019047193950133218,\n",
       " 0.0019274060816145578,\n",
       " -0.0019500238466097185,\n",
       " 0.0019725713029615623,\n",
       " -0.0,\n",
       " -0.002017449638282335,\n",
       " -0.0,\n",
       " 0.002062029573679981,\n",
       " -0.0,\n",
       " 0.002106299250628345,\n",
       " 0.0021283139396760195,\n",
       " -0.002150246455875678,\n",
       " -0.0021720952154822763,\n",
       " -0.0,\n",
       " -0.002215535012431828,\n",
       " -0.002237122763826933,\n",
       " -0.0,\n",
       " 0.0022800255751369643,\n",
       " -0.0023013372018134796,\n",
       " -0.002322553311657441,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0024064265887986506,\n",
       " -0.0,\n",
       " -0.0024477425949361305,\n",
       " 0.002468238767371621,\n",
       " -0.0024886244374227214,\n",
       " -0.0025088976085357404,\n",
       " -0.0,\n",
       " -0.0025490983253064928,\n",
       " -0.0025690217345441203,\n",
       " 0.0025888243714183327,\n",
       " -0.0,\n",
       " -0.00262805873172271,\n",
       " 0.0026474860820052456,\n",
       " 0.0026667839130761184,\n",
       " -0.002685949962153255,\n",
       " -0.002704981935492308,\n",
       " 0.0027238775081242375,\n",
       " -0.0,\n",
       " 0.002761249993767218,\n",
       " -0.0027797220984797535,\n",
       " -0.0027980481854145882,\n",
       " -0.0,\n",
       " -0.00283425233430581,\n",
       " 0.002852125328625078,\n",
       " -0.002869842169481069,\n",
       " -0.0028874002403296625,\n",
       " -0.0029047968911992193,\n",
       " -0.002922029438514402,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0029727151160297773,\n",
       " -0.0029892637358950535,\n",
       " -0.003005634325015385,\n",
       " -0.0,\n",
       " 0.0030378298237550756,\n",
       " -0.0030536488534447024,\n",
       " -0.0,\n",
       " 0.0030847145140766754,\n",
       " 0.0030999550572586246,\n",
       " -0.0,\n",
       " 0.0031298360897138184,\n",
       " -0.003144470283068232,\n",
       " 0.003158896006157028,\n",
       " -0.003173110024499421,\n",
       " -0.0031871090691404373,\n",
       " -0.0,\n",
       " 0.0032144489897722364,\n",
       " -0.0032277831565360655,\n",
       " 0.003240888931469459,\n",
       " -0.0032537628752762134,\n",
       " 0.003266401515166429,\n",
       " 0.0032788013451034975,\n",
       " -0.0,\n",
       " -0.0033028703864294,\n",
       " -0.0,\n",
       " -0.003325941297250813,\n",
       " -0.0,\n",
       " -0.003347984864279315,\n",
       " -0.0033586121280898477,\n",
       " 0.0033689713760160964,\n",
       " -0.0,\n",
       " -0.003388870637885945,\n",
       " -0.0033984029864389488,\n",
       " -0.0,\n",
       " -0.003416613745124829,\n",
       " -0.0,\n",
       " 0.003433659773370654,\n",
       " -0.0,\n",
       " -0.0034495093375987297,\n",
       " -0.003456975422592343,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.003483687059676639,\n",
       " -0.0,\n",
       " -0.0034950937882090526,\n",
       " 0.0035002953869957393,\n",
       " 0.0035051569120042475,\n",
       " 0.003509674173634911,\n",
       " -0.0,\n",
       " 0.0035176590898095238,\n",
       " -0.0,\n",
       " -0.0035242164045843046,\n",
       " 0.0035269491359866956,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.0035329127508759526,\n",
       " -0.0,\n",
       " 0.003534983922396658,\n",
       " -0.0035354354201867816,\n",
       " 0.003535491885922462,\n",
       " -0.0,\n",
       " -0.0035344028527431693,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.003521218889519658,\n",
       " -0.0,\n",
       " -0.0035134084970306965,\n",
       " -0.003508844579616147,\n",
       " 0.003503836205968303,\n",
       " 0.0034983794411848923,\n",
       " -0.0034924703911112056,\n",
       " 0.0034861052052431716,\n",
       " -0.0034792800796875655,\n",
       " 0.003471991260183322,\n",
       " -0.0,\n",
       " -0.0034560077889623586,\n",
       " -0.0,\n",
       " 0.003438125868478204,\n",
       " -0.0,\n",
       " 0.003418317572492199,\n",
       " -0.0034076826054102354,\n",
       " -0.0033965560779310503,\n",
       " -0.0,\n",
       " 0.0033728157743621065,\n",
       " -0.0033601959251284884,\n",
       " -0.003347072376404621,\n",
       " -0.0,\n",
       " -0.003319303038806112,\n",
       " -0.0,\n",
       " -0.00328948647387153,\n",
       " -0.003273804283999208,\n",
       " 0.0032576030708956947,\n",
       " -0.0,\n",
       " 0.0032236350172148297,\n",
       " -0.003205864201620583,\n",
       " 0.0031875664204484367,\n",
       " -0.0,\n",
       " -0.0031493834314638945,\n",
       " 0.003129495301056038,\n",
       " 0.0031090743675624027,\n",
       " 0.0030881195293097742,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0029989444494327136,\n",
       " -0.0,\n",
       " 0.0029511362699084966,\n",
       " 0.002926426383781223,\n",
       " -0.0029011795185010037,\n",
       " -0.002875396061839456,\n",
       " -0.0028490765848196253,\n",
       " -0.0,\n",
       " -0.002794832789954729,\n",
       " 0.00276691055996417,\n",
       " 0.0027384564915565557,\n",
       " -0.002709472120247831,\n",
       " -0.0,\n",
       " -0.0026499196232660006,\n",
       " -0.0,\n",
       " -0.002588269440079104,\n",
       " -0.0025566637477515705,\n",
       " -0.0025245412980161776,\n",
       " 0.0024919050934103025,\n",
       " -0.002458758355049952,\n",
       " 0.0024251045244519627,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.002285494912515765,\n",
       " -0.0,\n",
       " 0.002212753483234194,\n",
       " 0.002175665189845084,\n",
       " -0.0021381054345357706,\n",
       " -0.002100079696460451,\n",
       " -0.0,\n",
       " 0.002022653333500032,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.001903169048463234,\n",
       " 0.0018624752743491059,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.0016955617857809585,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0014781964827002966,\n",
       " -0.001433644550657372,\n",
       " 0.0013887539770563336,\n",
       " 0.0013435341502193736,\n",
       " 0.0012979946493818562,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0011595566672155876,\n",
       " -0.0011128379260963426,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0009233792857906676,\n",
       " 0.0008754229876927933,\n",
       " 0.0008272522568339492,\n",
       " -0.0007788784423079124,\n",
       " -0.0007303130189001238,\n",
       " -0.0,\n",
       " 0.000632653833042006,\n",
       " 0.0005835835880822227,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.000435553411913008,\n",
       " 0.00038597714642036424,\n",
       " -0.0003363047678268197,\n",
       " 0.0002865485664704196,\n",
       " -0.00023672088603872812,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -3.6943676670266e-05,\n",
       " 1.305492500384927e-05,\n",
       " 0.0,\n",
       " 0.000113029841532317,\n",
       " 0.00016298117376831167,\n",
       " 0.00021289179169119087,\n",
       " -0.0002627492531151027,\n",
       " -0.0003125411497937575,\n",
       " 0.0003622551152970694,\n",
       " 0.0,\n",
       " 0.00046140004298154025,\n",
       " -0.0005108065514082722,\n",
       " 0.0005600862364146474,\n",
       " 0.0006092270564491267,\n",
       " 0.0,\n",
       " -0.0007070443803107253,\n",
       " -0.0007556972676312011,\n",
       " -0.000804164071122461,\n",
       " -0.0008524332582773319,\n",
       " -0.0009004934191291152,\n",
       " 0.0009483332728086099,\n",
       " 0.0,\n",
       " -0.0010433076188500008,\n",
       " 0.001090420251628141,\n",
       " -0.00113726886995843,\n",
       " 0.0,\n",
       " -0.001230132055795855,\n",
       " 0.0,\n",
       " -0.0013218148399893952,\n",
       " 0.0013671886124242195,\n",
       " -0.0014122376849860748,\n",
       " -0.0014569525776128925,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0016322875862468027,\n",
       " 0.0016751962690864548,\n",
       " -0.001717718076279499,\n",
       " 0.0,\n",
       " 0.0018015690649263447,\n",
       " 0.0,\n",
       " 0.0018837787271535691,\n",
       " 0.0,\n",
       " -0.0019642887610332588,\n",
       " -0.002003889175445204,\n",
       " -0.0020430444438433748,\n",
       " -0.002081748291952377,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.002268292619554079,\n",
       " 0.0,\n",
       " -0.002339556411979308,\n",
       " 0.0023744504538715685,\n",
       " -0.002408846890830473,\n",
       " 0.0,\n",
       " 0.0024761311964089537,\n",
       " 0.002509011747388903,\n",
       " 0.0,\n",
       " -0.002573233012958945,\n",
       " 0.0,\n",
       " -0.0026353815375476175,\n",
       " -0.002665671970343739,\n",
       " -0.00269543678069039,\n",
       " 0.0,\n",
       " 0.0027533815767317622,\n",
       " 0.002781558095925173,\n",
       " -0.002809202054142278,\n",
       " 0.0028363122176608513,\n",
       " -0.0028628875472278864,\n",
       " 0.0028889271949405397,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.00301107351272034,\n",
       " 0.0030338915649516258,\n",
       " -0.0030561730787995746,\n",
       " 0.0030779186155570175,\n",
       " -0.0030991288965442473,\n",
       " 0.0031198047993728937,\n",
       " 0.003139947354178328,\n",
       " -0.0031595577398280785,\n",
       " 0.0,\n",
       " -0.003197187439879196,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.003266128293979355,\n",
       " 0.003282058193025247,\n",
       " 0.0,\n",
       " 0.0033123668041424867,\n",
       " -0.003326750316163871,\n",
       " 0.0,\n",
       " 0.0033539885582046687,\n",
       " 0.0,\n",
       " -0.0033792066800969645,\n",
       " -0.003391065399721839,\n",
       " -0.0034024279143514027,\n",
       " 0.0034132973474236927,\n",
       " -0.003423676903319943,\n",
       " 0.0034335698638504036,\n",
       " -0.003442979584787535,\n",
       " -0.0034519094924441046,\n",
       " 0.0034603630803016704,\n",
       " 0.0,\n",
       " -0.003475855586504319,\n",
       " 0.00348290179801881,\n",
       " 0.0,\n",
       " -0.0034956127820924827,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.003515616469369295,\n",
       " 0.0,\n",
       " 0.0035229721052027025,\n",
       " 0.0,\n",
       " 0.003528606516286167,\n",
       " 0.0,\n",
       " 0.003532552477489424,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0035355119404688828,\n",
       " 0.0035352486604162984,\n",
       " 0.003534592555458564,\n",
       " -0.00353354785769216,\n",
       " -0.0035321188047436895,\n",
       " 0.0035303096378267868,\n",
       " 0.00352812459984997,\n",
       " 0.003525567933591123,\n",
       " -0.0035226438799242183,\n",
       " -0.0035193566761128653,\n",
       " 0.0035157105541514266,\n",
       " -0.0035117097391689552,\n",
       " -0.0035073584478866517,\n",
       " -0.003502660887132817,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0034674378496447997,\n",
       " 0.0,\n",
       " -0.003453115168877198,\n",
       " -0.0034454881902228887,\n",
       " -0.0034375561768901293,\n",
       " 0.0034293231436722145,\n",
       " -0.003420793081611902,\n",
       " 0.0034119699571564348,\n",
       " -0.0034028577113436763,\n",
       " 0.0,\n",
       " -0.003383781488201623,\n",
       " -0.003373825259211128,\n",
       " 0.0,\n",
       " -0.003353095726469961,\n",
       " -0.0033423299998733245,\n",
       " 0.003331301968316498,\n",
       " 0.0033200153452319515,\n",
       " -0.0033084738131124534,\n",
       " -0.003296681023069516,\n",
       " 0.0032846405944302924,\n",
       " 0.0,\n",
       " -0.0032598311375029413,\n",
       " 0.003247069185691931,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.003193720900970517,\n",
       " 0.0,\n",
       " 0.0031657140193482288,\n",
       " 0.0031513890423233363,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0031071668741311175,\n",
       " 0.0030920210117900246,\n",
       " -0.003076677796056853,\n",
       " -0.0030611403047386483,\n",
       " 0.0,\n",
       " 0.003029494633098174,\n",
       " -0.0030133924349859315,\n",
       " -0.00299710792579485,\n",
       " 0.002980644010188556,\n",
       " 0.002964003558384663,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0029130511733919356,\n",
       " 0.0028957325937002997,\n",
       " 0.002878251316030177,\n",
       " -0.0028606100063875667,\n",
       " 0.00284281129726656,\n",
       " -0.0028248577878257053,\n",
       " 0.0028067520440781916,\n",
       " 0.002788496599086115,\n",
       " -0.0027700939531710156,\n",
       " 0.0,\n",
       " 0.0027328568974323577,\n",
       " 0.0027140273265047646,\n",
       " 0.0026950602329117797,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.002598242732503615,\n",
       " 0.002578498554444215,\n",
       " -0.002558632584473803,\n",
       " 0.0,\n",
       " 0.002518543743872541,\n",
       " -0.0024983250374568016,\n",
       " 0.002477992868140041,\n",
       " -0.0024575492460205363,\n",
       " 0.0024369961529271175,\n",
       " 0.0,\n",
       " -0.0023955693415942847,\n",
       " -0.0023746994483857076,\n",
       " 0.0023537277348797134,\n",
       " 0.0023326560461187965,\n",
       " 0.0023114862007145116,\n",
       " 0.002290219991160406,\n",
       " -0.002268859184145138,\n",
       " -0.0022474055208673406,\n",
       " -0.002225860717351784,\n",
       " -0.002204226464763749,\n",
       " -0.0021825044297280223,\n",
       " 0.0021606962546441098,\n",
       " 0.0,\n",
       " 0.0021168279347157726,\n",
       " 0.002094770956405445,\n",
       " 0.002072634171751646,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.002005760128822921,\n",
       " -0.001983319157547441,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0018928453828487945,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0018013016264340738,\n",
       " -0.0017782584189168788,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0017087715941294034,\n",
       " 0.0016854943322069636,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0016153342332869394,\n",
       " 0.0015918419466245074,\n",
       " -0.001568298750726188,\n",
       " 0.0,\n",
       " -0.0015210640417542972,\n",
       " 0.0014973746952056976,\n",
       " 0.0014736387730458914,\n",
       " -0.0014498573210867734,\n",
       " 0.0,\n",
       " -0.0014021619389576001,\n",
       " -0.0013782500291944556,\n",
       " -0.0013542966310041236,\n",
       " -0.001330302720655864,\n",
       " -0.0013062692613426397,\n",
       " 0.0012821972034535785,\n",
       " -0.001258087484845353,\n",
       " 0.0,\n",
       " -0.0012097587558516003,\n",
       " 0.001185541560937015,\n",
       " 0.0,\n",
       " -0.001137005962576822,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.0010151168620767,\n",
       " -0.0009906514565468975,\n",
       " 0.0,\n",
       " -0.0009416395821023372,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.000794011880861291,\n",
       " 0.0,\n",
       " 0.0007446255850790528,\n",
       " -0.0007199025687761374,\n",
       " 0.0006951606035616178,\n",
       " 0.0006704004037907244,\n",
       " 0.0006456226775694012,\n",
       " -0.0006208281269853247,\n",
       " 0.0005960174483436113,\n",
       " 0.0005711913323976713,\n",
       " 0.0,\n",
       " 0.0005214955252275521,\n",
       " -0.0004966271898228646,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.00039703323980714976,\n",
       " -0.00037210790205346214,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.00022238036159858863,\n",
       " 0.00019740250004426682,\n",
       " -0.000172419683948076,\n",
       " 0.0,\n",
       " 0.0001224417107315427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.745332304006714e-05,\n",
       " 2.245398607086195e-05,\n",
       " -0.0,\n",
       " -2.7545747023539897e-05,\n",
       " -0.0,\n",
       " -7.754272496974351e-05,\n",
       " 0.0001025386170999296,\n",
       " -0.0,\n",
       " -0.00015252207517693219,\n",
       " -0.0,\n",
       " 0.0002024902435149358,\n",
       " -0.0,\n",
       " 0.00025243807699479,\n",
       " 0.00027740278398509093,\n",
       " 0.00030236050207838906,\n",
       " -0.0003273105914843743,\n",
       " 0.0003522524097530156,\n",
       " 0.0003771853115531554,\n",
       " -0.0,\n",
       " 0.0004270217686772604,\n",
       " 0.0004519240169243438,\n",
       " -0.0,\n",
       " 0.0005016932571154879,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0005762489655351322,\n",
       " -0.0006010719930710283,\n",
       " 0.0006258794430535088,\n",
       " -0.0006506706236173705,\n",
       " 0.0006754448372989839,\n",
       " -0.0007002013808030762,\n",
       " 0.0007249395447703735,\n",
       " 0.0007496586135434398,\n",
       " 0.0007743578649311521,\n",
       " -0.000799036569971816,\n",
       " -0.0008236939926976974,\n",
       " -0.0,\n",
       " 0.0008729420108585326,\n",
       " 0.0008975310971637756,\n",
       " 0.0009220958824113525,\n",
       " -0.0009466355919885653,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.001044527880091059,\n",
       " 0.00106893028740194,\n",
       " -0.0010933027850456012,\n",
       " -0.0011176445339670033,\n",
       " -0.0,\n",
       " 0.0011662323780839865,\n",
       " 0.0011904767430591513,\n",
       " -0.001214686898352567,\n",
       " -0.001238861951242663,\n",
       " -0.001263000997490777,\n",
       " 0.0012871031210758492,\n",
       " -0.0013111673939288696,\n",
       " -0.0,\n",
       " -0.001359178613301038,\n",
       " -0.0013831236410118853,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.0014784768704521193,\n",
       " -0.001502203391693856,\n",
       " 0.001525883122521714,\n",
       " 0.0015495149993324177,\n",
       " -0.0,\n",
       " -0.0015966308607268237,\n",
       " -0.0,\n",
       " -0.0016435421599685423,\n",
       " -0.0,\n",
       " 0.001690239825751467,\n",
       " -0.001713505639173048,\n",
       " 0.0017367145219988576,\n",
       " -0.0017598652642951055,\n",
       " -0.0,\n",
       " 0.0018059873985682652,\n",
       " -0.0,\n",
       " -0.0018518620023229438,\n",
       " -0.0,\n",
       " -0.0018974787375334531,\n",
       " 0.001920187089811142,\n",
       " -0.0019428269580223957,\n",
       " 0.0019653969616250867,\n",
       " -0.001987895699578678,\n",
       " -0.0,\n",
       " -0.0020326736700075282,\n",
       " -0.0020549499950916245,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.002121310428647269,\n",
       " -0.0,\n",
       " 0.002165144902169336,\n",
       " -0.0,\n",
       " 0.002208639954936402,\n",
       " -0.0022302561250948346,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0022945590767310237,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.0023580072781455704,\n",
       " -0.0023789584040749137,\n",
       " -0.0,\n",
       " -0.0024205521806099774,\n",
       " -0.0024411910495630786,\n",
       " -0.0,\n",
       " -0.0024821430895087507,\n",
       " 0.002502452313407495,\n",
       " 0.0025226476615096897,\n",
       " -0.0,\n",
       " -0.002562688523319431,\n",
       " -0.0,\n",
       " 0.002602248969944633,\n",
       " 0.002621843688435892,\n",
       " 0.002641311824380331,\n",
       " 0.002660651155418383,\n",
       " 0.002679859428576891,\n",
       " -0.0,\n",
       " -0.002717873634697565,\n",
       " -0.002736674906274108,\n",
       " -0.0,\n",
       " 0.00277385389603347,\n",
       " -0.0027922267622460463,\n",
       " -0.0028104519209317352,\n",
       " -0.0,\n",
       " 0.0028464490550493267,\n",
       " -0.0,\n",
       " 0.002881824847964266,\n",
       " -0.0,\n",
       " 0.0029165583183583914,\n",
       " 0.002933677479344858,\n",
       " 0.0029506279482329504,\n",
       " 0.002967406950574195,\n",
       " 0.0029840116777763892,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0030486304619806496,\n",
       " -0.003064320481941288,\n",
       " -0.0030798186524387777,\n",
       " -0.00309512192329887,\n",
       " 0.0031102272096459587,\n",
       " 0.003125131391905153,\n",
       " 0.0031398313158215794,\n",
       " -0.0031543237924997196,\n",
       " -0.0031686055984581005,\n",
       " -0.0,\n",
       " -0.003196524131876256,\n",
       " 0.003210154240268932,\n",
       " -0.0032235604400741853,\n",
       " 0.0032367393364989825,\n",
       " -0.003249687500965541,\n",
       " 0.003262401471326909,\n",
       " -0.0032748777521063046,\n",
       " -0.0032871128147697903,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0033223349191963264,\n",
       " 0.003333569173761538,\n",
       " 0.0033445440829663158,\n",
       " -0.0,\n",
       " -0.0033657009562161177,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.003395397207960738,\n",
       " -0.0034047368875269324,\n",
       " -0.0,\n",
       " 0.0034225543213412344,\n",
       " 0.003431024209056922,\n",
       " -0.0,\n",
       " -0.00344706646931003,\n",
       " -0.0,\n",
       " -0.0034618852860340064,\n",
       " 0.003468825784381098,\n",
       " -0.0034754482367799692,\n",
       " 0.003481748538905784,\n",
       " 0.003487722567831915,\n",
       " 0.0034933661831649592,\n",
       " 0.003498675228224391,\n",
       " 0.0035036455312834544,\n",
       " -0.0,\n",
       " 0.0035125531569961013,\n",
       " 0.003516482072765978,\n",
       " -0.0035200554356014445,\n",
       " 0.0035232690188485476,\n",
       " -0.0,\n",
       " -0.003528599908845889,\n",
       " -0.0035307087360587582,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.003534757837270553,\n",
       " -0.003535334277308081,\n",
       " -0.0,\n",
       " -0.0,\n",
       " 0.0035346846003144355,\n",
       " -0.0035336610019871357,\n",
       " -0.0035322268954335614,\n",
       " -0.0,\n",
       " 0.0035281104985595466,\n",
       " -0.0035254199285332678,\n",
       " -0.0035223022957100504,\n",
       " 0.003518753521078367,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0,\n",
       " -0.0035001664920131393,\n",
       " -0.0034944019287512598,\n",
       " -0.0,\n",
       " -0.0034815042405410297,\n",
       " 0.0034743635261256043,\n",
       " -0.0,\n",
       " -0.00345867976639473,\n",
       " -0.0034501294514099334,\n",
       " -0.0034411020998127255,\n",
       " 0.0034315942315305556]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(np.arange(3), p=np.array([0.2, 0.6, 0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar1 = np.arange(10)\n",
    "ar1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar2 = np.arange(2,7)\n",
    "ar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sushantkumar/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:1: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([       inf, 2.        , 1.5       , 1.33333333, 1.25      ,\n",
       "       1.2       , 1.16666667, 1.14285714, 1.125     ])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar1[1:]/ar1[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(l1)\n",
    "\n",
    "some_tensor = tf.convert_to_tensor(np.arange(8), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_tensor = tf.reshape(some_tensor, shape=(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model_14_4/dense_42/Softmax:0' shape=(1, 3) dtype=float32>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model(reshaped_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(2,8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
